----

# THIS FILE WAS GENERATED AUTOMATICALLY.
# CHANGES MADE HERE WILL BE OVERWRITTEN.

template: pages/speaker.html.njk
title: 'Charlie Gerard: Look mum, no hands!'
speaker:
  published: true
  reviewed: true
  id: charlie-gerard-look-mum-no-hands
  order: 0
  firstname: Charlie
  lastname: Gerard
  talkTitle: 'Look mum, no hands!'
  twitterHandle: '@devdevcharlie'
  githubHandle: '@charliegerard'
  links:
    twitter: 'https://twitter.com/devdevcharlie'
    github: 'https://github.com/charliegerard'
    homepage: 'http://charliegerard.github.io/'
  country: Australia
  city: Sydney
  name: Charlie Gerard
  image:
    filename: charlie-gerard.jpg
    width: 440
    height: 440

----

A typical interaction with a device or interface involves touching it. Either
you're pressing buttons on a controller, swiping on a touchscreen or clicking
on your laptop's trackpad. But what if you could control things without the use
of your hands? What if you could use... your thoughts?

I have been tinkering with a brain sensor and developed an open source
JavaScript framework for it to allow me (or anyone else) to control interfaces
or robots using facial expressions and mental commands.